{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN-1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Mount the drive that contains the data."
      ],
      "metadata": {
        "id": "AgOgA3NQxthi"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXDTj-V7kj0M"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import and load all the required packages."
      ],
      "metadata": {
        "id": "Sosk5coux-FL"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "663u4moEksh6"
      },
      "source": [
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, Flatten, Dense, Input, BatchNormalization, Dropout, MaxPool2D\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import os\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Specify the path to the file containing the data.\n",
        "Split the available data into training data, testing data and validation data and reset the index."
      ],
      "metadata": {
        "id": "7B6gMRkLyGCe"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrjZ1eLfkzrB"
      },
      "source": [
        "path = \"/content/drive/MyDrive/Mixed Images\"\n",
        "filenames = os.listdir(path)\n",
        "\n",
        "df=pd.DataFrame({'filename':filenames})\n",
        "df[\"category\"] = df.apply(lambda x: x['filename'].split('_')[0], axis=1)\n",
        "\n",
        "temp1 = df[df.category=='Damaged']\n",
        "temp2 = df[df.category=='Undamaged']\n",
        "df = pd.concat([temp1, temp2],ignore_index=True, axis = 0)\n",
        "df.category.value_counts()\n",
        "\n",
        "train_df, validate_df = train_test_split(df, test_size=0.30, random_state=42, stratify=df[\"category\"])\n",
        "validate_df, test_df = train_test_split(validate_df, test_size=0.5, random_state=42, stratify=validate_df[\"category\"])\n",
        "\n",
        "\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "validate_df = validate_df.reset_index(drop=True)\n",
        "test_df = test_df.reset_index(drop=True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHjFE8NplEHb",
        "outputId": "908f52d3-0c9b-4bc9-e093-a12f767c562c"
      },
      "source": [
        "print(train_df.category.value_counts())\n",
        "print(validate_df.category.value_counts())\n",
        "print(test_df.category.value_counts())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Damaged      1074\n",
            "Undamaged    1028\n",
            "Name: category, dtype: int64\n",
            "Damaged      230\n",
            "Undamaged    220\n",
            "Name: category, dtype: int64\n",
            "Damaged      230\n",
            "Undamaged    221\n",
            "Name: category, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building the convolutional layers for the CNN model."
      ],
      "metadata": {
        "id": "DkUCWGOsyv5h"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIIrAblGlGwQ"
      },
      "source": [
        "cnn= tf.keras.models.Sequential()\n",
        "\n",
        "#First convolutional layer\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=256,kernel_size=(3,3),activation='relu',input_shape=[224,224,3]))\n",
        "cnn.add(BatchNormalization())\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
        "cnn.add(Dropout(0.2))\n",
        "\n",
        "#Second convolutional layer\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=512,kernel_size=(3,3),activation='relu'))\n",
        "cnn.add(BatchNormalization())\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
        "cnn.add(Dropout(0.2))\n",
        "\n",
        "#Third convolutional layer\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=512,kernel_size=(3,3),activation='relu'))\n",
        "cnn.add(BatchNormalization())\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
        "cnn.add(Dropout(0.2))\n",
        "\n",
        "#Fourth convolutional layer\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=256,kernel_size=(3,3),activation='relu'))\n",
        "cnn.add(BatchNormalization())\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
        "cnn.add(Dropout(0.2))\n",
        "\n",
        "#Flattening\n",
        "cnn.add(tf.keras.layers.Flatten())\n",
        "cnn.add(Dropout(0.2))\n",
        "\n",
        "#Fully connected layer\n",
        "cnn.add(tf.keras.layers.Dense(128,activation='relu'))\n",
        "cnn.add(Dropout(0.5))\n",
        "\n",
        "#Output layer\n",
        "cnn.add(tf.keras.layers.Dense(units=len(train_df.category.value_counts()),activation='softmax'))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image augmentation using the Image data generator function."
      ],
      "metadata": {
        "id": "rEcYkHE_020D"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUEajZYVlK1A"
      },
      "source": [
        "train_datagen = ImageDataGenerator( rotation_range=15,\n",
        "                                    rescale=1./255,\n",
        "                                    shear_range=0.1,\n",
        "                                    zoom_range=0.2,\n",
        "                                    horizontal_flip=True,\n",
        "                                    vertical_flip = True,\n",
        "                                    width_shift_range=0.1,\n",
        "                                    height_shift_range=0.1)\n",
        "\n",
        "train_set = train_datagen.flow_from_dataframe(train_df,path,x_col='filename',y_col='category',\n",
        "                                              target_size=(224, 224),class_mode='categorical',batch_size=32)\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "validation_set = validation_datagen.flow_from_dataframe(validate_df,path,x_col='filename',\n",
        "                                                        y_col='category',target_size=(224, 224),\n",
        "                                                        class_mode='categorical',batch_size=32)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "test_set = test_datagen.flow_from_dataframe(test_df,path,x_col='filename',\n",
        "                                            y_col='category',target_size=(224, 224),\n",
        "                                            class_mode='categorical',batch_size=32, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Specify the loss function and the hyperparameters."
      ],
      "metadata": {
        "id": "AxcC7GMM1pDf"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNDNjb1glTYf"
      },
      "source": [
        "cnn.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=1e-04), metrics=['accuracy'])\n",
        "cnn.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fit the model and use early stopping callback to stop training once the model performance stops improving."
      ],
      "metadata": {
        "id": "ozpyBmfd2W2v"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uj89WQMcl0q4"
      },
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=5, restore_best_weights=True)\n",
        "\n",
        "r = cnn.fit(train_set, \n",
        "            validation_data=validation_set,\n",
        "            epochs=15,\n",
        "            steps_per_epoch=len(train_set),\n",
        "            validation_steps=len(validation_set),\n",
        "            callbacks=callback)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot loss and accuracy graphs to visualize the performance of the model."
      ],
      "metadata": {
        "id": "V_-feZA53P5P"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wQNNWhCt09X"
      },
      "source": [
        "plt.plot(r.history['loss'], label='train loss')\n",
        "plt.plot(r.history['val_loss'], label='val loss')\n",
        "plt.legend()\n",
        "plt.savefig('/content/drive/MyDrive/CNN_ValLoss_2.png')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQDK8VeKt50k"
      },
      "source": [
        "plt.plot(r.history['accuracy'], label='train acc')\n",
        "plt.plot(r.history['val_accuracy'], label='val acc')\n",
        "plt.legend()\n",
        "plt.savefig('/content/drive/MyDrive/CNN_ValACC_2.png')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NllWCdnFuADr"
      },
      "source": [
        "train_set.class_indices.items()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the trained model on the drive."
      ],
      "metadata": {
        "id": "ogwQbLJS3z-A"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQLzZdquuME7"
      },
      "source": [
        "cnn.save('/content/drive/MyDrive/CNN.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the saved model and make predictions on the test dataset."
      ],
      "metadata": {
        "id": "KtFBFGv938Bs"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVVsVZiGu2R5"
      },
      "source": [
        "cnn = tf.keras.models.load_model('/content/drive/MyDrive/CNN.h5')\n",
        "test_set.reset()\n",
        "pred = cnn.predict(test_set)\n",
        "test_df[\"pred\"] = np.argmax(pred, axis=1)\n",
        "test_df[\"pred\"] = test_df[\"pred\"].replace({0:'Damaged',1:'Undamaged'})\n",
        "test_df.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLH67dk_8N6M"
      },
      "source": [
        "test_df.to_csv(\"/content/drive/MyDrive/test_df.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzOeao789jUc"
      },
      "source": [
        "cnn.evaluate(test_set, verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot a confusion matrix to understand the model predictions."
      ],
      "metadata": {
        "id": "iCuS9kqD5CNm"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrgpJKqyqALP"
      },
      "source": [
        "cm = confusion_matrix(test_df['category'], test_df['pred'])\n",
        "sns.heatmap(cm, annot=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evBF4GgSqDKG"
      },
      "source": [
        "target_names = ['Damaged','Undamaged']\n",
        "classification_report(test_df['category'], test_df['pred'], target_names= target_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualise the predictions made by the model."
      ],
      "metadata": {
        "id": "h3RmJSRz5PVu"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OMU8NFgqL7M"
      },
      "source": [
        "from keras.preprocessing import image\n",
        "\n",
        "sample_test = test_df.sample(n=35).reset_index(drop=True)\n",
        "plt.figure(figsize=(20, 20))\n",
        "for index, row in sample_test.iterrows():\n",
        "    filename = row['filename']\n",
        "    pred = row['pred']\n",
        "    img = image.load_img(path + \"/\" + filename, target_size=(224,224))\n",
        "    plt.subplot(5, 7, index+1)\n",
        "    plt.imshow(img)\n",
        "    plt.xlabel(filename.split(' ')[0] + '(' + \"{}\".format(pred) + ')' )\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example of Image augmentation."
      ],
      "metadata": {
        "id": "dpOIwNe97XbX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLA753DvqPp0"
      },
      "source": [
        "# Image Augmentation\n",
        "example_df = train_df.sample(n=1).reset_index(drop=True)\n",
        "example_set = train_datagen.flow_from_dataframe(\n",
        "    example_df, \n",
        "    path, \n",
        "    x_col='filename',\n",
        "    y_col='category',\n",
        "    target_size=(224,224),\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(12, 12))\n",
        "for i in range(0, 15):\n",
        "    plt.subplot(5, 3, i+1)\n",
        "    for X_batch, Y_batch in example_set:\n",
        "        image = X_batch[0]\n",
        "        plt.imshow(image)\n",
        "        break\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}